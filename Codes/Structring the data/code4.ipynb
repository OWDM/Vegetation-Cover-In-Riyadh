{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data has been successfully written to the HDF5 file.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "# Paths to the data folders and files\n",
    "ndvi_folder = \"C:\\\\Users\\\\Musae\\\\Documents\\\\GitHub-REPOs\\\\Senior-project_Doc\\\\Docs\\\\Array\\\\NDVI-Array\"\n",
    "ndmi_folder = \"C:\\\\Users\\\\Musae\\\\Documents\\\\GitHub-REPOs\\\\Senior-project_Doc\\\\Docs\\\\Array\\\\NDMI-Array\"\n",
    "csv_path = \"C:\\\\Users\\\\Musae\\\\Documents\\\\GitHub-REPOs\\\\Senior-project_Doc\\\\monthly_averages_formatted.csv\"\n",
    "\n",
    "# Load CSV data\n",
    "climate_data = pd.read_csv(csv_path)\n",
    "\n",
    "# Create or open HDF5 file\n",
    "with h5py.File('environmental_data.h5', 'w') as hdf:\n",
    "    # Create groups for each type of data\n",
    "    climate_grp = hdf.create_group('Climate')\n",
    "    ndvi_grp = hdf.create_group('NDVI')\n",
    "    ndmi_grp = hdf.create_group('NDMI')\n",
    "    \n",
    "    # Add CSV data to the Climate group\n",
    "    for column in climate_data.columns:\n",
    "        climate_grp.create_dataset(column, data=climate_data[column].to_numpy())\n",
    "\n",
    "    # Function to add data to HDF5\n",
    "    def add_data_to_group(group, folder_path, file_prefix):\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith(\".npy\"):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                data = np.load(file_path)\n",
    "                dataset_name = file_prefix + \"_\" + filename.split('.')[0]\n",
    "                group.create_dataset(dataset_name, data=data)\n",
    "\n",
    "    # Add NDVI data to the NDVI group\n",
    "    add_data_to_group(ndvi_grp, ndvi_folder, 'NDVI')\n",
    "\n",
    "    # Add NDMI data to the NDMI group\n",
    "    add_data_to_group(ndmi_grp, ndmi_folder, 'NDMI')\n",
    "\n",
    "print(\"All data has been successfully written to the HDF5 file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"C:\\\\Users\\\\Musae\\\\Documents\\\\GitHub-REPOs\\\\Vegetation-Cover-In-Riyadh\\\\Codes\\\\Structring the data\\\\environmental_data.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing completed and updated in HDF5 file.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Open the existing HDF5 file\n",
    "hdf5_file_path = \"C:\\\\Users\\\\Musae\\\\Documents\\\\GitHub-REPOs\\\\Vegetation-Cover-In-Riyadh\\\\Codes\\\\Structring the data\\\\environmental_data.h5\"\n",
    "with h5py.File(hdf5_file_path, 'a') as hdf:\n",
    "    # Load climate data\n",
    "    temp = hdf['Climate']['Temp Average'][:]  # Adjusted dataset name\n",
    "    precip = hdf['Climate']['PRECTOTCORR Average'][:]  # Adjusted dataset name\n",
    "\n",
    "    # Convert to DataFrame for easier handling\n",
    "    climate_data = pd.DataFrame({\n",
    "        'Temperature': temp,\n",
    "        'Precipitation': precip\n",
    "    })\n",
    "\n",
    "    # Handle missing values by filling with the median\n",
    "    climate_data.fillna(climate_data.median(), inplace=True)\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    climate_scaled = scaler.fit_transform(climate_data)\n",
    "\n",
    "    # Replace the original datasets with scaled data\n",
    "    del hdf['Climate']['Temp Average'], hdf['Climate']['PRECTOTCORR Average']\n",
    "    climate_grp = hdf['Climate']\n",
    "    climate_grp.create_dataset('Temperature_Scaled', data=climate_scaled[:, 0])\n",
    "    climate_grp.create_dataset('Precipitation_Scaled', data=climate_scaled[:, 1])\n",
    "\n",
    "print(\"Data preprocessing completed and updated in HDF5 file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 0.00576519238459658\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "with h5py.File(hdf5_file_path, 'r') as hdf:\n",
    "    # Initialize lists to store features and labels\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    # Assuming climate data has the same length as NDVI datasets and is aligned\n",
    "    temp_scaled = hdf['Climate']['Temperature_Scaled'][:]\n",
    "    precip_scaled = hdf['Climate']['Precipitation_Scaled'][:]\n",
    "    \n",
    "    # Load and prepare NDVI features\n",
    "    ndvi_group = hdf['NDVI']\n",
    "    sorted_datasets = sorted(ndvi_group.keys())\n",
    "    for i in range(1, len(sorted_datasets)):\n",
    "        current_ndvi = ndvi_group[sorted_datasets[i]][:]\n",
    "        previous_ndvi = ndvi_group[sorted_datasets[i-1]][:]\n",
    "\n",
    "        # Combine current climate and previous NDVI into features\n",
    "        # Flatten NDVI data if necessary or use summary statistics like mean, max\n",
    "        combined_features = np.hstack([temp_scaled[i], precip_scaled[i], np.mean(previous_ndvi)])\n",
    "        features.append(combined_features)\n",
    "        labels.append(np.mean(current_ndvi))\n",
    "\n",
    "# Convert lists to numpy arrays for training\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Calculate and print the root mean squared error\n",
    "rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation RMSE scores: [0.00902148 0.00756931 0.00320033 0.00672428 0.0055663 ]\n",
      "Mean RMSE: 0.006416337858432081\n",
      "Standard deviation: 0.001963478473652581\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Assuming 'features' and 'labels' are your full dataset and 'model' is your RandomForestRegressor\n",
    "scores = cross_val_score(model, features, labels, scoring='neg_mean_squared_error', cv=5)\n",
    "rmse_scores = np.sqrt(-scores)\n",
    "\n",
    "print(\"Cross-validation RMSE scores:\", rmse_scores)\n",
    "print(\"Mean RMSE:\", rmse_scores.mean())\n",
    "print(\"Standard deviation:\", rmse_scores.std())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
