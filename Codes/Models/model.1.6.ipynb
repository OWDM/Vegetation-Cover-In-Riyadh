{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "import os\n",
    "\n",
    "# Set paths to the images and masks\n",
    "image_dir = 'N:\\\\My Drive\\\\Data\\\\RUH'\n",
    "mask_dir = 'N:\\\\My Drive\\\\Data\\\\Mask'\n",
    "# List of image and mask files\n",
    "image_files = sorted(os.listdir(image_dir))[:3000]\n",
    "mask_files = sorted(os.listdir(mask_dir))[:3000]\n",
    "\n",
    "# Function to normalize images and encode masks\n",
    "def prepare_data(img_path, mask_path):\n",
    "    # Read the image and mask files\n",
    "    img = imread(img_path) / 255.0  # Normalize to [0, 1]\n",
    "    mask = imread(mask_path, as_gray=True)  # Read mask as grayscale\n",
    "    # Resize images and masks if not already 256x256\n",
    "    if img.shape[0] != 256 or img.shape[1] != 256:\n",
    "        img = resize(img, (256, 256), anti_aliasing=True)\n",
    "    if mask.shape[0] != 256 or mask.shape[1] != 256:\n",
    "        mask = resize(mask, (256, 256), order=0, preserve_range=True)\n",
    "    # Map mask pixel values to class labels\n",
    "    mask[mask == 255] = 3\n",
    "    mask[mask == 170] = 2\n",
    "    mask[mask == 85] = 1\n",
    "    mask[mask == 0] = 0\n",
    "    # Convert mask to categorical\n",
    "    mask = to_categorical(mask, num_classes=4)\n",
    "    return img, mask\n",
    "\n",
    "# Generator to load images in batches\n",
    "def generate_batches(image_files, mask_files, batch_size):\n",
    "    num_samples = len(image_files)\n",
    "    while True:  # Loop indefinitely\n",
    "        for start in range(0, num_samples, batch_size):\n",
    "            end = min(start + batch_size, num_samples)\n",
    "            images, masks = [], []\n",
    "            for i in range(start, end):\n",
    "                img_path = os.path.join(image_dir, image_files[i])\n",
    "                mask_path = os.path.join(mask_dir, mask_files[i])\n",
    "                img, mask = prepare_data(img_path, mask_path)\n",
    "                images.append(img)\n",
    "                masks.append(mask)\n",
    "            yield np.array(images), np.array(masks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split the dataset into training and validation sets\n",
    "image_files_train, image_files_val, mask_files_train, mask_files_val = train_test_split(\n",
    "    image_files, mask_files, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Training and validation generator\n",
    "train_generator = generate_batches(image_files_train, mask_files_train, batch_size)\n",
    "val_generator = generate_batches(image_files_val, mask_files_val, batch_size)\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def unet_model(input_size=(256, 256, 3), num_classes=4):\n",
    "    inputs = Input(input_size)\n",
    "    # Encoder\n",
    "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)\n",
    "    c1 = Dropout(0.1)(c1)\n",
    "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
    "    c2 = Dropout(0.1)(c2)\n",
    "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
    "    c3 = Dropout(0.2)(c3)\n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n",
    "    c4 = Dropout(0.2)(c4)\n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
    "    p4 = MaxPooling2D(pool_size=(2, 2))(c4)\n",
    "\n",
    "    # Bottleneck\n",
    "    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n",
    "    c5 = Dropout(0.3)(c5)\n",
    "    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
    "\n",
    "    # Decoder\n",
    "    u6 = UpSampling2D((2, 2))(c5)\n",
    "    u6 = Concatenate()([u6, c4])\n",
    "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n",
    "    c6 = Dropout(0.2)(c6)\n",
    "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n",
    "\n",
    "    u7 = UpSampling2D((2, 2))(c6)\n",
    "    u7 = Concatenate()([u7, c3])\n",
    "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
    "    c7 = Dropout(0.2)(c7)\n",
    "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
    "\n",
    "    u8 = UpSampling2D((2, 2))(c7)\n",
    "    u8 = Concatenate()([u8, c2])\n",
    "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n",
    "    c8 = Dropout(0.1)(c8)\n",
    "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
    "    u9 = UpSampling2D((2, 2))(c8)\n",
    "    u9 = Concatenate()([u9, c1])\n",
    "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n",
    "    c9 = Dropout(0.1)(c9)\n",
    "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
    "\n",
    "    # Output layer\n",
    "    outputs = Conv2D(num_classes, (1, 1), activation='softmax')(c9)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create the U-Net model\n",
    "unet = unet_model()\n",
    "\n",
    "# Display the model architecture\n",
    "unet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Function to compute weighted accuracy\n",
    "def weighted_accuracy(y_true, y_pred):\n",
    "    # Flatten and convert to argmax\n",
    "    y_true_f = tf.keras.backend.flatten(tf.keras.backend.argmax(y_true, axis=-1))\n",
    "    y_pred_f = tf.keras.backend.flatten(tf.keras.backend.argmax(y_pred, axis=-1))\n",
    "\n",
    "    # Define class weights (adjust these values to reflect your class distribution)\n",
    "    class_weights = np.array([0.1, 0.5, 1.5, 3.0], dtype='float32')\n",
    "\n",
    "    # Create a boolean mask for each class and calculate weights\n",
    "    weights = tf.gather(class_weights, y_true_f)\n",
    "    matches = tf.keras.backend.cast(y_true_f == y_pred_f, 'float32')\n",
    "    weighted_acc = tf.keras.backend.sum(matches * weights) / tf.keras.backend.sum(weights)\n",
    "    return weighted_acc\n",
    "\n",
    "# Function to compute mean IoU\n",
    "def mean_iou(num_classes):\n",
    "    def mean_iou_metric(y_true, y_pred):\n",
    "        # Convert predictions to one-hot encoded arrays\n",
    "        y_pred = tf.keras.backend.argmax(y_pred, axis=-1)\n",
    "        y_true = tf.keras.backend.argmax(y_true, axis=-1)\n",
    "\n",
    "        # Initialize IoU variables\n",
    "        iou_scores = []\n",
    "\n",
    "        for i in range(num_classes):\n",
    "            # Create binary masks for the current class\n",
    "            pred_mask = tf.keras.backend.cast(y_pred == i, 'float32')\n",
    "            true_mask = tf.keras.backend.cast(y_true == i, 'float32')\n",
    "\n",
    "            # Calculate intersection and union\n",
    "            intersection = tf.keras.backend.sum(pred_mask * true_mask)\n",
    "            union = tf.keras.backend.sum(pred_mask) + tf.keras.backend.sum(true_mask) - intersection\n",
    "\n",
    "            # Ensure all calculations are float32\n",
    "            intersection = tf.keras.backend.cast(intersection, 'float32')\n",
    "            union = tf.keras.backend.cast(union, 'float32')\n",
    "\n",
    "            # Avoid division by zero\n",
    "            iou = (intersection + tf.keras.backend.epsilon()) / (union + tf.keras.backend.epsilon())\n",
    "            iou_scores.append(iou)\n",
    "\n",
    "        # Compute mean IoU\n",
    "        mean_iou_value = tf.keras.backend.mean(tf.stack(iou_scores))\n",
    "        return mean_iou_value\n",
    "\n",
    "    return mean_iou_metric\n",
    "\n",
    "# Update model compilation to include both metrics\n",
    "num_classes = 4  # Adjust this according to your use case\n",
    "unet.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[mean_iou(num_classes), weighted_accuracy, 'accuracy']\n",
    ")# Function to compute weighted accuracy\n",
    "def weighted_accuracy(y_true, y_pred):\n",
    "    # Flatten and convert to argmax\n",
    "    y_true_f = tf.keras.backend.flatten(tf.keras.backend.argmax(y_true, axis=-1))\n",
    "    y_pred_f = tf.keras.backend.flatten(tf.keras.backend.argmax(y_pred, axis=-1))\n",
    "\n",
    "    # Define class weights (adjust these values to reflect your class distribution)\n",
    "    class_weights = np.array([0.1, 0.5, 1.5, 3.0], dtype='float32')\n",
    "\n",
    "    # Create a boolean mask for each class and calculate weights\n",
    "    weights = tf.gather(class_weights, y_true_f)\n",
    "    matches = tf.keras.backend.cast(y_true_f == y_pred_f, 'float32')\n",
    "    weighted_acc = tf.keras.backend.sum(matches * weights) / tf.keras.backend.sum(weights)\n",
    "    return weighted_acc\n",
    "\n",
    "# Function to compute mean IoU\n",
    "def mean_iou(num_classes):\n",
    "    def mean_iou_metric(y_true, y_pred):\n",
    "        # Convert predictions to one-hot encoded arrays\n",
    "        y_pred = tf.keras.backend.argmax(y_pred, axis=-1)\n",
    "        y_true = tf.keras.backend.argmax(y_true, axis=-1)\n",
    "\n",
    "        # Initialize IoU variables\n",
    "        iou_scores = []\n",
    "\n",
    "        for i in range(num_classes):\n",
    "            # Create binary masks for the current class\n",
    "            pred_mask = tf.keras.backend.cast(y_pred == i, 'float32')\n",
    "            true_mask = tf.keras.backend.cast(y_true == i, 'float32')\n",
    "\n",
    "            # Calculate intersection and union\n",
    "            intersection = tf.keras.backend.sum(pred_mask * true_mask)\n",
    "            union = tf.keras.backend.sum(pred_mask) + tf.keras.backend.sum(true_mask) - intersection\n",
    "\n",
    "            # Ensure all calculations are float32\n",
    "            intersection = tf.keras.backend.cast(intersection, 'float32')\n",
    "            union = tf.keras.backend.cast(union, 'float32')\n",
    "\n",
    "            # Avoid division by zero\n",
    "            iou = (intersection + tf.keras.backend.epsilon()) / (union + tf.keras.backend.epsilon())\n",
    "            iou_scores.append(iou)\n",
    "\n",
    "        # Compute mean IoU\n",
    "        mean_iou_value = tf.keras.backend.mean(tf.stack(iou_scores))\n",
    "        return mean_iou_value\n",
    "\n",
    "    return mean_iou_metric\n",
    "\n",
    "# Update model compilation to include both metrics\n",
    "num_classes = 4  # Adjust this according to your use case\n",
    "unet.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[mean_iou(num_classes), weighted_accuracy, 'accuracy']\n",
    ")\n",
    "\n",
    "# Define callbacks for saving the model and early stopping\n",
    "model_checkpoint = ModelCheckpoint('unet_segmentation_4.keras', monitor='val_loss', save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "# Training the model\n",
    "history = unet.fit(\n",
    "    x=train_generator,\n",
    "    y=None,  # Since the generator yields both images and masks, y is not separately provided\n",
    "    batch_size=None,  # Batch size is handled by the generator\n",
    "    epochs=10,\n",
    "    steps_per_epoch=int(np.ceil(len(image_files_train) / batch_size)),  # Convert to int\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=int(np.ceil(len(image_files_val) / batch_size)),  # Convert to int\n",
    "    callbacks=[model_checkpoint, early_stopping]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
