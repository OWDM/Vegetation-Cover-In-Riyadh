{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy Score': 0.9665489196777344, 'F1 Score': 0.6525545036379322, 'Jaccard Score': 0.6390441999307992, 'Precision Score': 0.6411742552999673, 'Recall Score': 0.6644099812908941}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Musae\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, jaccard_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Load images\n",
    "actual_mask = np.array(Image.open(\"N:\\\\My Drive\\\\Data\\\\ActualMasks\\\\actual_20.png\"))\n",
    "predicted_mask = np.array(Image.open(\"N:\\\\My Drive\\\\Data\\\\Predictions\\\\prediction_20.png\"))\n",
    "\n",
    "# Ensure the masks are in categorical format if they are not\n",
    "# For example, if your masks are in class indices (0 to 3):\n",
    "# actual_mask = (actual_mask == np.arange(4)[:, None, None]).astype(int)\n",
    "# predicted_mask = (predicted_mask == np.arange(4)[:, None, None]).astype(int)\n",
    "\n",
    "# Metrics calculation\n",
    "metrics = {\n",
    "    \"Accuracy Score\": accuracy_score(actual_mask.flatten(), predicted_mask.flatten()),\n",
    "    \"F1 Score\": f1_score(actual_mask.flatten(), predicted_mask.flatten(), average='macro'),  # Use average='macro' for imbalanced classes\n",
    "    \"Jaccard Score\": jaccard_score(actual_mask.flatten(), predicted_mask.flatten(), average='macro'),\n",
    "    \"Precision Score\": precision_score(actual_mask.flatten(), predicted_mask.flatten(), average='macro'),\n",
    "    \"Recall Score\": recall_score(actual_mask.flatten(), predicted_mask.flatten(), average='macro')\n",
    "}\n",
    "\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, jaccard_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Define directories\n",
    "actual_dir = \"N:\\\\My Drive\\\\Data\\\\ActualMasks\"\n",
    "predicted_dir = 'N:\\\\My Drive\\\\Data\\\\Predictions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Musae\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Musae\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Musae\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Musae\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Musae\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Musae\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Musae\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Musae\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Musae\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Musae\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Musae\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Average Accuracy Score': 0.9746871590614319, 'Average F1 Score': 0.8271213627052977, 'Average Jaccard Score': 0.7877927002714582, 'Average Precision Score': 0.9208912079876994, 'Average Recall Score': 0.8111405522731121}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, jaccard_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Define directories\n",
    "actual_dir = \"N:\\\\My Drive\\\\Data\\\\ActualMasks\"\n",
    "predicted_dir = 'N:\\\\My Drive\\\\Data\\\\Predictions'\n",
    "\n",
    "# Initialize lists to store scores\n",
    "accuracies = []\n",
    "f1_scores = []\n",
    "jaccard_scores = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "missing_files = []\n",
    "\n",
    "# Iterate over actual masks\n",
    "for filename in os.listdir(actual_dir):\n",
    "    actual_path = os.path.join(actual_dir, filename)\n",
    "    predicted_filename = filename.replace('actual_', 'prediction_')  # Adjusted to match prediction filenames\n",
    "    predicted_path = os.path.join(predicted_dir, predicted_filename)\n",
    "    \n",
    "    # Check if the predicted file exists\n",
    "    if not os.path.exists(predicted_path):\n",
    "        missing_files.append(filename)\n",
    "        continue\n",
    "    \n",
    "    # Load masks\n",
    "    actual_mask = np.array(Image.open(actual_path))\n",
    "    predicted_mask = np.array(Image.open(predicted_path))\n",
    "\n",
    "    # Compute metrics for each image\n",
    "    accuracies.append(accuracy_score(actual_mask.flatten(), predicted_mask.flatten()))\n",
    "    f1_scores.append(f1_score(actual_mask.flatten(), predicted_mask.flatten(), average='macro', zero_division=1))\n",
    "    jaccard_scores.append(jaccard_score(actual_mask.flatten(), predicted_mask.flatten(), average='macro'))\n",
    "    precisions.append(precision_score(actual_mask.flatten(), predicted_mask.flatten(), average='macro', zero_division=1))\n",
    "    recalls.append(recall_score(actual_mask.flatten(), predicted_mask.flatten(), average='macro'))\n",
    "\n",
    "# Calculate average scores\n",
    "average_metrics = {\n",
    "    \"Average Accuracy Score\": np.mean(accuracies),\n",
    "    \"Average F1 Score\": np.mean(f1_scores),\n",
    "    \"Average Jaccard Score\": np.mean(jaccard_scores),\n",
    "    \"Average Precision Score\": np.mean(precisions),\n",
    "    \"Average Recall Score\": np.mean(recalls)\n",
    "}\n",
    "\n",
    "# Output results\n",
    "print(average_metrics)\n",
    "if missing_files:\n",
    "    print(\"Missing predicted files for the following actual masks:\", missing_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 6000.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing: prediction_0.png with actual_22.png\n",
      "Comparing: prediction_1.png with actual_23.png\n",
      "Comparing: prediction_2.png with actual_24.png\n",
      "Comparing: prediction_3.png with actual_25.png\n",
      "Comparing: prediction_4.png with actual_26.png\n",
      "Comparing: prediction_5.png with actual_27.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for pred_path, true_path in tqdm(zip(pred_mask_files, true_mask_files), total=len(pred_mask_files)):\n",
    "    print(f\"Comparing: {os.path.basename(pred_path)} with {os.path.basename(true_path)}\")\n",
    "    # Rest of your code for processing and metrics calculation...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([  0, 128, 255], dtype=uint8), array([63020,  2502,    14], dtype=int64))\n",
      "(array([  0, 128, 255], dtype=uint8), array([65272,   263,     1], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(true_mask, return_counts=True))\n",
    "print(np.unique(pred_mask, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Mask file not found: N:\\My Drive\\Data\\ActualMasks",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Musae\\Documents\\GitHub-REPOs\\Vegetation-Cover-In-Riyadh\\Codes\\Models\\metric.ipynb Cell 4\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Musae/Documents/GitHub-REPOs/Vegetation-Cover-In-Riyadh/Codes/Models/metric.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m your_actual_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mN:\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mMy Drive\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mData\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mActualMasks\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Musae/Documents/GitHub-REPOs/Vegetation-Cover-In-Riyadh/Codes/Models/metric.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Assuming true_mask is your actual mask loaded properly\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Musae/Documents/GitHub-REPOs/Vegetation-Cover-In-Riyadh/Codes/Models/metric.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m true_mask \u001b[39m=\u001b[39m load_and_preprocess_mask(your_actual_path)  \u001b[39m# Load a correct mask\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Musae/Documents/GitHub-REPOs/Vegetation-Cover-In-Riyadh/Codes/Models/metric.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Create a completely random prediction mask\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Musae/Documents/GitHub-REPOs/Vegetation-Cover-In-Riyadh/Codes/Models/metric.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m random_pred_mask \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39m4\u001b[39m, size\u001b[39m=\u001b[39mtrue_mask\u001b[39m.\u001b[39msize)  \u001b[39m# Assuming 4 classes\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\Musae\\Documents\\GitHub-REPOs\\Vegetation-Cover-In-Riyadh\\Codes\\Models\\metric.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Musae/Documents/GitHub-REPOs/Vegetation-Cover-In-Riyadh/Codes/Models/metric.ipynb#W5sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m mask \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(path, cv2\u001b[39m.\u001b[39mIMREAD_GRAYSCALE)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Musae/Documents/GitHub-REPOs/Vegetation-Cover-In-Riyadh/Codes/Models/metric.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mif\u001b[39;00m mask \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Musae/Documents/GitHub-REPOs/Vegetation-Cover-In-Riyadh/Codes/Models/metric.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMask file not found: \u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Musae/Documents/GitHub-REPOs/Vegetation-Cover-In-Riyadh/Codes/Models/metric.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mif\u001b[39;00m mask\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m (height, width):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Musae/Documents/GitHub-REPOs/Vegetation-Cover-In-Riyadh/Codes/Models/metric.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     mask \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mresize(mask, (width, height))  \u001b[39m# Resize only if necessary\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Mask file not found: N:\\My Drive\\Data\\ActualMasks"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "your_actual_path = \"N:\\\\My Drive\\\\Data\\\\ActualMasks\"\n",
    "# Assuming true_mask is your actual mask loaded properly\n",
    "true_mask = load_and_preprocess_mask(your_actual_path)  # Load a correct mask\n",
    "\n",
    "# Create a completely random prediction mask\n",
    "random_pred_mask = np.random.randint(0, 4, size=true_mask.size)  # Assuming 4 classes\n",
    "\n",
    "# Calculate metrics\n",
    "acc = accuracy_score(true_mask, random_pred_mask)\n",
    "f1 = f1_score(true_mask, random_pred_mask, average='weighted', zero_division=0)\n",
    "jac = jaccard_score(true_mask, random_pred_mask, average='weighted', zero_division=0)\n",
    "recall = recall_score(true_mask, random_pred_mask, average='weighted', zero_division=0)\n",
    "precision = precision_score(true_mask, random_pred_mask, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"Random Test - Accuracy: {acc:.5f}, F1 Score: {f1:.5f}, Jaccard: {jac:.5f}, Recall: {recall:.5f}, Precision: {precision:.5f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
